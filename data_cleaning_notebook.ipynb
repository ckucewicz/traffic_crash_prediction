{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d974edc4",
   "metadata": {},
   "source": [
    "# Cleaning Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e9344",
   "metadata": {},
   "source": [
    "## 1.0 Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c26a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for getting data\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59979ca3",
   "metadata": {},
   "source": [
    "## 2.0 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2641d",
   "metadata": {},
   "source": [
    "### 2.1 Crashes\n",
    "\n",
    "2.1.1 Preview Data: .head()\n",
    "\n",
    "2.1.2 Understand Structure: .info()\n",
    "\n",
    "2.1.3 Format Feature names and Row Values: .lower()\n",
    "\n",
    "2.1.4 Drop features with overly high null values: .isna().sum()/ len(df) for percentage of nulls for each feature\n",
    "\n",
    "2.1.5 Check for duplicates: .duplicated().sum()\n",
    "\n",
    "2.1.6 Keep or drop features with remaining nulls:\n",
    "\n",
    "2.1.7 Inspect remaining features: .value_counts()\n",
    "\n",
    "2.1.8 Remove unuseful features: .drop() for list of features deemed not useful for analysis; store trimmed df as ‘df_name_cleaned’\n",
    "\n",
    "2.1.9 Reduce feature cardinality with bucketing:\n",
    "\n",
    "2.1.10 Convert data types: stored data types to reflect true data types (categorical variables as strings, numeric variables as int, etc.)\n",
    "\n",
    "2.1.11 Remove remaining nulls: .dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138dbaf9",
   "metadata": {},
   "source": [
    "Steps:\n",
    "* **Preview Data**: `.head()`\n",
    "\n",
    "\n",
    "* **Understand Structure**: `.info()`\n",
    "\n",
    "\n",
    "* **Format Feature names and Row Values**: `.lower()`\n",
    "\n",
    "\n",
    "* **Drop features with overly high null values**: `.isna().sum()/ len(df)` for percentage of nulls for each feature\n",
    "\n",
    "\n",
    "* **Check for duplicates**: `.duplicated().sum()`\n",
    "\n",
    "\n",
    "* **Keep or drop features with remaining nulls**:\n",
    "\n",
    "    * make intentional decisions to keep or drop using `.value_counts()` distribution and domain knowledge\n",
    "\n",
    "\n",
    "* **inspect remaining features**: `.value_counts()`; \n",
    "\n",
    "    * make intentional decisions to keep or drop using `.value_counts()` distribution and domain knowledge; \n",
    "    * make note of any features to keep that will need cleaning/cardinality reduction/etc.\n",
    "\n",
    "\n",
    "* **remove unuseful features**: \n",
    "\n",
    "    * `.drop()` for list of features deemed not useful for analysis; \n",
    "    * store trimmed df as 'df_name_cleaned'\n",
    "    \n",
    "\n",
    "* **reduce feature cardinality with bucketing**:\n",
    "\n",
    "    * `trafficway_type` and `lane_cnt`\n",
    "    * `crash_hour`, `crash_day_of_week`, `crash_month`\n",
    "    * `posted_speed_limit`\n",
    "    * `traffic_control_device`\n",
    "    * `prim_contributory_cause`\n",
    "    * `most_severe_injury`\n",
    " \n",
    " \n",
    "* **Convert data types**: \n",
    "\n",
    "    * stored data types to reflect true data types \n",
    "    * (text variables as strings, numeric variables as int, categorical as category, ect.)\n",
    "    \n",
    " \n",
    "* **remove remaining nulls**: `.dropna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes = pd.read_csv('./data/traffic_crashes.csv', low_memory = True)\n",
    "crashes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43896ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes.columns = crashes.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string values in object columns to lowercase\n",
    "for col in crashes.select_dtypes(include='object').columns:\n",
    "    crashes[col] = crashes[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de2e66d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_percentage = round((crashes.isna().sum()/len(crashes)*100), 2)\n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting all features with 90% or more of its values are null\n",
    "high_null_features = crashes.columns[(crashes.isna().sum() / len(crashes) * 100) >= 90]\n",
    "high_null_features\n",
    "\n",
    "# creating a list of features with 90% or more null values\n",
    "high_null_features_list = list(high_null_features)\n",
    "high_null_features_list\n",
    "\n",
    "crashes_cleaned = crashes.drop(columns=high_null_features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting all features with 90% or more of its values are null\n",
    "high_null_features = crashes.columns[(crashes.isna().sum() / len(crashes) * 100) >= 90]\n",
    "high_null_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of features with 90% or more null values\n",
    "high_null_features_list = list(high_null_features)\n",
    "high_null_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4720f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned = crashes.drop(columns=high_null_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate crash_record_id values in crashes_cleaned\n",
    "print(f\"Number of duplicate crash_record_id values in crashes_cleaned: {crashes_cleaned['crash_record_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_null_features = crashes.columns[\n",
    "    ((crashes.isna().sum() / len(crashes) * 100) >= 60) &\n",
    "    ((crashes.isna().sum() / len(crashes) * 100) < 90)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e1f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_null_features_list = list(medium_null_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ff5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in medium_null_features_list:\n",
    "    print(f\"Value counts for column '{feature}':\")\n",
    "    print(crashes_cleaned[feature].value_counts())\n",
    "    print(\"-\"* 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1bbef6",
   "metadata": {},
   "source": [
    "hit_and_run_i is an aftermath, not a contributor so we can remove that\n",
    "\n",
    "lane_cnt could be important. Will need to reduce cardinality though.\n",
    "\n",
    "intersection_related_i... could be helpful. A little vague and subjective according to CDOT description. \"A field observation by the police officer whether an intersection played a role in the crash. Does not represent whether or not the crash occurred within the intersection.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11559bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['latitude'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae9d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['longitude'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae911be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368bf27",
   "metadata": {},
   "source": [
    "While location data would be very helpful, the cardinality is simply too high and will restrict my limited computing power. I'll remove lat, long, and location. I'll see "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efdd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['posted_speed_limit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144eb45e",
   "metadata": {},
   "source": [
    "This feature could be important, but will require cardinality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize speed limits directly without using a function\n",
    "crashes_cleaned['speed_limit_category'] = pd.cut(\n",
    "    crashes_cleaned['posted_speed_limit'],\n",
    "    bins=[-float('inf'), 25, 40, float('inf')],\n",
    "    labels=['Low', 'Medium', 'High'],\n",
    "    right=True\n",
    ")\n",
    "\n",
    "# Check the result\n",
    "crashes_cleaned['speed_limit_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a262699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['traffic_control_device'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ae03f",
   "metadata": {},
   "source": [
    "Could be important but will need to reduce the cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map the original 'traffic_control_device' values to more specific categories\n",
    "traffic_control_mapping = {\n",
    "    'traffic signal': 'Signal',\n",
    "    'flashing control signal': 'Signal',\n",
    "    'pedestrian crossing sign': 'Signal',  # If it's a signal\n",
    "    'railroad crossing gate': 'Signal',    # If it uses lights\n",
    "    \n",
    "    'stop sign/flasher': 'Sign',\n",
    "    'yield': 'Sign',\n",
    "    'school zone': 'Sign',\n",
    "    'railroad crossing sign': 'Sign',      # If static sign\n",
    "    'other warning sign': 'Sign',\n",
    "    'bicycle crossing sign': 'Sign',\n",
    "    'no passing': 'Sign',\n",
    "    \n",
    "    'lane use marking': 'Markings & Lanes',\n",
    "    'delineators': 'Markings & Lanes',\n",
    "    \n",
    "    'no controls': 'Other',\n",
    "    'unknown': 'Other',\n",
    "    'other': 'Other',\n",
    "    'police/flagman': 'Other',\n",
    "    'other railroad crossing': 'Other'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'traffic_control_device' column\n",
    "crashes_cleaned['traffic_control_category'] = crashes_cleaned['traffic_control_device'].map(traffic_control_mapping)\n",
    "\n",
    "# Check the value counts for the new grouped categories\n",
    "crashes_cleaned['traffic_control_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba0bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['device_condition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe05a2",
   "metadata": {},
   "source": [
    "The top two categories that make up the majority of this feature are no controls and functioning properly. And then the next two frequent are unknown and other. We can drop this. won't be useful for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae75a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['weather_condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee63b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['lighting_condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b832ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['first_crash_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc01b1",
   "metadata": {},
   "source": [
    "This seems like it could be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['trafficway_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419de0db",
   "metadata": {},
   "source": [
    "This is important will keep this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define intersection types\n",
    "intersection_types = ['roundabout', 'l-intersection', 'y-intersection', \n",
    "                      'five point, or more', 'center turn lane', \n",
    "                      't-intersection', 'unknown intersection type']\n",
    "\n",
    "# Define conditions for both blocks (with block 2 modification)\n",
    "conditions = [\n",
    "    (crashes_cleaned['trafficway_type'] == 'one-way') & (crashes_cleaned['lane_cnt'] == 1),\n",
    "    (crashes_cleaned['trafficway_type'] == 'one-way') & (crashes_cleaned['lane_cnt'] > 1),\n",
    "    (crashes_cleaned['trafficway_type'].isin(intersection_types)),\n",
    "    (crashes_cleaned['trafficway_type'].isin(['unknown', 'not reported'])) | \n",
    "    (pd.isnull(crashes_cleaned['trafficway_type'])) | \n",
    "    (pd.isnull(crashes_cleaned['lane_cnt'])),\n",
    "    (crashes_cleaned['trafficway_type'].isin(['parking lot', 'driveway', 'ramp', 'alley', 'other'])),\n",
    "    # Modified condition for 'multi-lane bidirectional' from Block 2\n",
    "    (crashes_cleaned['lane_cnt'] > 1) & \n",
    "    (~crashes_cleaned['trafficway_type'].isin([\n",
    "        'one-way', 'four way', 'unknown', 'not reported', \n",
    "        'other', 'parking lot', 'driveway', 'ramp', 'alley'\n",
    "    ]))\n",
    "]\n",
    "\n",
    "# Define corresponding categories\n",
    "choices = [\n",
    "    'single-lane one way',\n",
    "    'multi-lane one way',\n",
    "    'intersection',\n",
    "    'unknown',  # Combined \"unknown\" and \"not reported\"\n",
    "    'other',\n",
    "    'multi-lane bidirectional'\n",
    "]\n",
    "\n",
    "# Apply classification\n",
    "crashes_cleaned['road_category'] = np.select(conditions, choices, default='unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd38b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of categories in the new column\n",
    "crashes_cleaned['road_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['alignment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc030024",
   "metadata": {},
   "source": [
    "While this feature would ideally be helpful in analysis, the data here is not conducive for analysis. Most of the entries are 'straight and level'. will remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['roadway_surface_cond'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a2803",
   "metadata": {},
   "source": [
    "This is somewhat redundant with weather condition. Will remove weather_condition  and keep roadway_surface_cond due to its lower cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['street_direction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['street_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['road_defect'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f608e",
   "metadata": {},
   "source": [
    "The main two categories here are \"no defects\" and unknown. This will not be helpful for analysis. Will remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['crash_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80527011",
   "metadata": {},
   "source": [
    "This describes the aftermath, not helpful for contributory factors. Will remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['beat_of_occurrence'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af94b4e4",
   "metadata": {},
   "source": [
    "High cardinality, unlikely to be useful for analysis. to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbed0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['most_severe_injury'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ad803",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['injuries_total'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d4beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['injuries_fatal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf331708",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['injuries_incapacitating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af588273",
   "metadata": {},
   "source": [
    "The 'injuries_...' features are redundant. This information is captured in the 'most_severe_injury' feature. Will remove all 'injuries_...' features and keep most_severe_injury. \n",
    "\n",
    "Most_sever_injury will require cardinality reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf959ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the string 'nan' with actual NaN values\n",
    "crashes_cleaned['most_severe_injury'] = crashes_cleaned['most_severe_injury'].replace('nan', np.nan)\n",
    "\n",
    "# Now categorize the injuries into 'Serious' and 'Non-serious'\n",
    "crashes_cleaned['severity_category'] = crashes_cleaned['most_severe_injury'].replace({\n",
    "    'no indication of injury': 'Non-serious',\n",
    "    'nonincapacitating injury': 'Non-serious',\n",
    "    'reported, not evident': 'Non-serious',\n",
    "    'incapacitating injury': 'Serious',\n",
    "    'fatal': 'Serious'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808eae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['prim_contributory_cause'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a01e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['sec_contributory_cause'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02097c4e",
   "metadata": {},
   "source": [
    "This feature is redundant to prim_contributory_cause. A high majority of values are either not applicable or unable to determine so it will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for the primary contributory causes\n",
    "cause_mapping = {\n",
    "    'distraction - from inside vehicle': 'Distraction',\n",
    "    'distraction - from outside vehicle': 'Distraction',\n",
    "    'cell phone use other than texting': 'Distraction',\n",
    "    'distraction - other electronic device (navigation device, dvd player, etc.)': 'Distraction',\n",
    "    'texting': 'Distraction',\n",
    "    'bicycle advancing legally on red light': 'Distraction',\n",
    "    'motorcycle advancing legally on red light': 'Distraction',\n",
    "    \n",
    "    'operating vehicle in erratic, reckless, careless, negligent or aggressive manner': 'Aggressive/Reckless Driving',\n",
    "    'failing to reduce speed to avoid crash': 'Aggressive/Reckless Driving',\n",
    "    'exceeding authorized speed limit': 'Aggressive/Reckless Driving',\n",
    "    'exceeding safe speed for conditions': 'Aggressive/Reckless Driving',\n",
    "    'driving on wrong side/wrong way': 'Aggressive/Reckless Driving',\n",
    "    'disregarding stop sign': 'Aggressive/Reckless Driving',\n",
    "    'disregarding traffic signals': 'Aggressive/Reckless Driving',\n",
    "    'disregarding yield sign': 'Aggressive/Reckless Driving',\n",
    "    'passing stopped school bus': 'Aggressive/Reckless Driving',\n",
    "    'improper overtaking/passing': 'Aggressive/Reckless Driving',\n",
    "    'failing to yield right-of-way': 'Aggressive/Reckless Driving',\n",
    "    'following too closely': 'Aggressive/Reckless Driving',\n",
    "    'improper lane usage': 'Aggressive/Reckless Driving',\n",
    "    'improper turning/no signal': 'Aggressive/Reckless Driving',\n",
    "    \n",
    "    'driving skills/knowledge/experience': 'Driver\\'s Condition/Experience',\n",
    "    'physical condition of driver': 'Driver\\'s Condition/Experience',\n",
    "    'vision obscured (signs, tree limbs, buildings, etc.)': 'Driver\\'s Condition/Experience',\n",
    "    'under the influence of alcohol/drugs (use when arrest is effected)': 'Driver\\'s Condition/Experience',\n",
    "    'had been drinking (use when arrest is not made)': 'Driver\\'s Condition/Experience',\n",
    "    \n",
    "    'weather': 'Environmental and Road Conditions',\n",
    "    'road engineering/surface/marking defects': 'Environmental and Road Conditions',\n",
    "    'road construction/maintenance': 'Environmental and Road Conditions',\n",
    "    'evasive action due to animal, object, nonmotorist': 'Environmental and Road Conditions',\n",
    "    'animal': 'Environmental and Road Conditions',\n",
    "    \n",
    "    'unable to determine': 'Unknown/Other',\n",
    "    'not applicable': 'Unknown/Other',\n",
    "    'related to bus stop': 'Unknown/Other',\n",
    "    'obstructed crosswalks': 'Unknown/Other',\n",
    "    \n",
    "    # Add the missing categories\n",
    "    'improper backing': 'Aggressive/Reckless Driving',\n",
    "    'equipment - vehicle condition': 'Driver\\'s Condition/Experience',\n",
    "    'disregarding other traffic signs': 'Aggressive/Reckless Driving',\n",
    "    'disregarding road markings': 'Aggressive/Reckless Driving',\n",
    "    'turning right on red': 'Aggressive/Reckless Driving'\n",
    "}\n",
    "\n",
    "# Apply the mapping to categorize the causes\n",
    "crashes_cleaned['crash_cause_category'] = crashes_cleaned['prim_contributory_cause'].map(cause_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe9d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique values in 'prim_contributory_cause' that are not in the 'cause_mapping'\n",
    "missing_values = crashes_cleaned[~crashes_cleaned['prim_contributory_cause'].isin(cause_mapping.keys())]['prim_contributory_cause'].unique()\n",
    "\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the value counts in the new category column\n",
    "crashes_cleaned['crash_cause_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214de34f",
   "metadata": {},
   "source": [
    "Damage, 'date_police_notified' both deal with aftermath, Not contributory factors. To be removed\n",
    "\n",
    "'street_no'... unhelpful. Will remove\n",
    "\n",
    "'num_units' not helpful in contributory factor, to remove\n",
    "\n",
    "remove latitude and longitude as these are captured in location feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a1d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['crash_date'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8cc792",
   "metadata": {},
   "source": [
    "Will remove this feature. This information is captured in crash_hour, crash_day_of_the_week, crash_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['crash_hour'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['crash_day_of_week'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['crash_month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4cb22b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crashes_cleaned['time_of_day'] = pd.cut(\n",
    "    crashes_cleaned['crash_hour'], \n",
    "    bins=[-1, 5, 11, 17, 23], \n",
    "    labels=['Night (Late)', 'Morning', 'Afternoon', 'Night (Early)'],\n",
    "    right=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0fe4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['day_of_week'] = crashes_cleaned['crash_day_of_week'].replace({\n",
    "    1: 'Sun',\n",
    "    2: 'Mon',\n",
    "    3: 'Tues',\n",
    "    4: 'Wed',\n",
    "    5: 'Thur',\n",
    "    6: 'Fri',\n",
    "    7: 'Sat'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['day_of_week'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['season'] = pd.cut(\n",
    "    crashes_cleaned['crash_month'], \n",
    "    bins=[0, 2, 5, 8, 11, 12], \n",
    "    labels=['Winter', 'Spring', 'Summer', 'Fall', 'Winter'],\n",
    "    right=True,\n",
    "    ordered=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee1e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542558d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned.drop(columns = [\n",
    "    'crash_date',\n",
    "    'hit_and_run_i',\n",
    "    'device_condition',\n",
    "    'weather_condition',\n",
    "    'road_defect',\n",
    "    'crash_type',\n",
    "    'damage',\n",
    "    'date_police_notified',\n",
    "    'sec_contributory_cause',\n",
    "    'street_no',\n",
    "    'report_type',\n",
    "    'beat_of_occurrence',\n",
    "    'num_units',\n",
    "    'alignment',\n",
    "    'injuries_total',\n",
    "    'injuries_fatal',\n",
    "     'injuries_incapacitating',\n",
    "     'injuries_non_incapacitating',\n",
    "     'injuries_reported_not_evident',\n",
    "     'injuries_no_indication',\n",
    "    'injuries_unknown',\n",
    "    'location',\n",
    "    'street_direction',\n",
    "    'lane_cnt', \n",
    "    'intersection_related_i',\n",
    "    'trafficway_type', \n",
    "    'crash_hour', \n",
    "    'crash_day_of_week', \n",
    "    'crash_month', \n",
    "    'posted_speed_limit', \n",
    "    'traffic_control_device', \n",
    "    'street_name', \n",
    "    'most_severe_injury',\n",
    "    'prim_contributory_cause',\n",
    "    'latitude',\n",
    "    'longitude'\n",
    "], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d3500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the columns (except 'crash_record_id') to category type\n",
    "crashes_cleaned[[col for col in crashes_cleaned.columns if col != 'crash_record_id']] = crashes_cleaned[[col for col in crashes_cleaned.columns if col != 'crash_record_id']].astype('category')\n",
    "\n",
    "# Verify the changes\n",
    "crashes_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb5868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(crashes_cleaned.isna().sum()/ len(crashes_cleaned))* 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['severity_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0355d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad941a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ac73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['severity_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(crashes_cleaned.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22989f5",
   "metadata": {},
   "source": [
    "### 2.2 People \n",
    "\n",
    "2.2.1 Preview data\n",
    "\n",
    "2.2.2 Understand Structure\n",
    "\n",
    "2.2.3 Format Feature names and Row Values\n",
    "\n",
    "2.2.4 Drop features with overly high null values\n",
    "\n",
    "2.2.5 Checking for duplicates\n",
    "\n",
    "2.2.6 Inspect remaining features\n",
    "\n",
    "2.2.7 remove unuseful feature\n",
    "\n",
    "2.2.8 convert data types\n",
    "\n",
    "2.2.9 reduce feature cardinality with bucketing\n",
    "\n",
    "2.2.10 remove remaining nulls\n",
    "\n",
    "2.2.11 Merge Preparation: Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504df7b",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "* Steps:\n",
    "* **Preview Data**: `.head()`\n",
    "\n",
    "\n",
    "* **Understand Structure**: `.info()`\n",
    "\n",
    "\n",
    "* **Format Feature names and Row Values**: `.lower()`\n",
    "\n",
    "\n",
    "* **Drop features with overly high null values**: `.isna().sum()/ len(df)` for percentage of nulls for each feature\n",
    "\n",
    "\n",
    "* **Check for duplicates**: `.duplicated().sum()`\n",
    "\n",
    "\n",
    "* **inspect remaining features**: `.value_counts()`; \n",
    "\n",
    "    * make intentional decisions to keep or drop using `.value_counts()` distribution and domain knowledge; \n",
    "    * make note of any features to keep that will need cleaning/cardinality reduction/etc.\n",
    "\n",
    "\n",
    "* **remove unuseful features**: \n",
    "\n",
    "    * `.drop()` for list of features deemed not useful for analysis; \n",
    "    * store trimmed df as 'df_name_cleaned'\n",
    "\n",
    "\n",
    "* **Convert data types**: \n",
    "\n",
    "    * stored data types to reflect true data types \n",
    "    * (categorical variables as strings, numeric variables as int, ect.)\n",
    "\n",
    "\n",
    "* **reduce feature cardinality with bucketing**:\n",
    "\n",
    "    * 'safety_equipment' to 'safety_equipment_category'\n",
    "    * 'age' to 'age_group'\n",
    " \n",
    " \n",
    "* **remove remaining nulls**: `.dropna()`\n",
    "\n",
    "\n",
    "* **Merge Preparation: Aggregation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6292105",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pd.read_csv('./data/people.csv', low_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0dceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "people.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b612a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "people.columns = people.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string values in object columns to lowercase\n",
    "for col in people.select_dtypes(include='object').columns:\n",
    "    people[col] = people[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f750c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "round((people.isna().sum()/ len(people)*100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b185ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting all features with 90% or more of its values are null\n",
    "ppl_high_null_features = people.columns[(people.isna().sum() / len(people) * 100) >= 90]\n",
    "ppl_high_null_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b22de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of features with 90% or more null values\n",
    "ppl_high_null_features_list = list(ppl_high_null_features)\n",
    "ppl_high_null_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d666cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned = people.drop(columns=ppl_high_null_features_list)\n",
    "people_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566cd814",
   "metadata": {},
   "source": [
    "#### Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb644e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the list of ID columns to check for duplicates\n",
    "id_columns = ['person_id', 'crash_record_id', 'vehicle_id']\n",
    "\n",
    "# Loop through each column to check for duplicates\n",
    "for column in id_columns:\n",
    "    \n",
    "    # Check for duplicates in the current column\n",
    "    duplicates = people_cleaned.duplicated(subset=[column])\n",
    "    \n",
    "    # Get the count of duplicate rows\n",
    "    duplicates_count = duplicates.sum()\n",
    "    \n",
    "    # Print the count of duplicates\n",
    "    print(f\"Number of duplicate {column} rows: {duplicates_count}\")\n",
    "    \n",
    "    # Optionally, view the actual duplicate rows for the current column\n",
    "    duplicate_rows = people_cleaned[duplicates]\n",
    "    print(f\"\\n{column} duplicate rows:\")\n",
    "    print(duplicate_rows)\n",
    "    print(\"\\n\" + \"=\"*50)  # Separator for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ffd9cf",
   "metadata": {},
   "source": [
    "##### Explanation of Output:\n",
    "\n",
    "1. **Duplicate person_id Rows:**\n",
    "   - No duplicate person_id rows were found, which means each person_id is unique in this dataset.\n",
    "\n",
    "2. **Duplicate crash_record_id Rows:**\n",
    "   - A total of 1,080,392 rows are duplicates based on crash_record_id. This suggests that multiple individuals (drivers, passengers, etc.) may have been associated with the same crash. This is expected if there are multiple people involved in the same crash event.\n",
    "\n",
    "3. **Duplicate vehicle_id Rows:**\n",
    "   - A total of 421,011 rows are duplicates based on vehicle_id. This indicates that some vehicles appear in multiple records, potentially due to different passengers or crashes involving the same vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1409717",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned[people_cleaned.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned['person_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea7003",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned['crash_date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2faae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned['seat_no'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f57cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned['safety_equipment'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85ff74",
   "metadata": {},
   "source": [
    "Could be an important predictor, but will need to reduce cardinality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map the original 'safety_equipment' values to broader categories\n",
    "safety_equipment_mapping = {\n",
    "    # Used Equipment\n",
    "    'safety belt used': 'Used',\n",
    "    'child restraint used': 'Used',\n",
    "    'child restraint - forward facing': 'Used',\n",
    "    'bicycle helmet (pedacyclist involved only)': 'Used',\n",
    "    'child restraint - type unknown': 'Used',\n",
    "    'child restraint - rear facing': 'Used',\n",
    "    'dot compliant motorcycle helmet': 'Used',\n",
    "    'helmet used': 'Used',\n",
    "    'booster seat': 'Used',\n",
    "    'child restraint used improperly': 'Used',\n",
    "\n",
    "    # Not Used Equipment\n",
    "    'safety belt not used': 'Not Used',\n",
    "    'helmet not used': 'Not Used',\n",
    "    'child restraint not used': 'Not Used',\n",
    "    'not dot compliant motorcycle helmet': 'Not Used',\n",
    "    'should/lap belt used improperly': 'Not Used',\n",
    "\n",
    "    # Unknown Equipment Usage\n",
    "    'usage unknown': 'Unknown',\n",
    "\n",
    "    # Other/Special Case Equipment\n",
    "    'none present': 'Other/Special Case', \n",
    "    'wheelchair': 'Other/Special Case',\n",
    "    'stretcher': 'Other/Special Case',\n",
    "    'unknown': 'Other/Special Case',  # Catch-all for any unknown or missing values\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'safety_equipment' column\n",
    "people_cleaned['safety_equipment_category'] = people_cleaned['safety_equipment'].map(safety_equipment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4cceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the value counts for the new grouped categories\n",
    "people_cleaned['safety_equipment_category'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e6cd4",
   "metadata": {},
   "source": [
    "Based on this output, even after recategorization, this feature will not be very useful. About 95% of the data is split between \"used\" and \"unknown\", with the remaining 5% split between \"other/special case\" and \"not used\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned['airbag_deployed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e00e3",
   "metadata": {},
   "source": [
    "This feature might be more helpful if we simply knew: did the airbag deploy or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ff8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for airbag_deployed\n",
    "airbag_mapping = {\n",
    "    'did not deploy': 'Not Deployed',\n",
    "    'not applicable': 'Not Deployed',  # Assuming \"not applicable\" should be considered as unknown\n",
    "    'deployment unknown': 'Unknown',\n",
    "    'deployed, front': 'Deployed',\n",
    "    'deployed, combination': 'Deployed',\n",
    "    'deployed, side': 'Deployed',\n",
    "    'deployed other (knee, air, belt, etc.)': 'Deployed'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'airbag_deployed' column\n",
    "people_cleaned['airbag_deployed'] = people_cleaned['airbag_deployed'].map(airbag_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36242551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, check the new value counts\n",
    "people_cleaned['airbag_deployed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3bd55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned['ejection'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3fbdcb",
   "metadata": {},
   "source": [
    "This feature might be normally be helpful, but it is far too skewed to be helpful for this analysis. Only about 8.5k values other than 'none' or 'unknown'. This will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6320864",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned['injury_classification'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5a636",
   "metadata": {},
   "source": [
    "We can drop this. It contains similar information to 'most_severe_injury' but is more imbalanced so I will drop it and keep 'most_severe_injury' as my target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33222b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(people_cleaned['driver_action'].value_counts(normalize = True), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a6482",
   "metadata": {},
   "source": [
    "similar to 'prim_contributory_cause' in crashes. but this one contains 20% nulls. Will drop this and keep prim_contributory_cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a862b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned['driver_vision'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c46c5",
   "metadata": {},
   "source": [
    "This feature is too skewed to provide any real analytical benefit. The top two by 500k values are 'not_obscured' and 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned['physical_condition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea49a5",
   "metadata": {},
   "source": [
    "Most are normal or unknown. Not particularly helpful. Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8481577",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned['bac_result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c9b66",
   "metadata": {},
   "source": [
    "most are test not offered and test refused. Again, not very helpful, so will drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned['age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720483f1",
   "metadata": {},
   "source": [
    "Could be interesting to investigate, but high cardinality means it will need some bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e05fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (replace with your actual DataFrame)\n",
    "age_bins_df = pd.DataFrame({\n",
    "    'age': [5, 15, 16, 25, 30, 60, 100, 120, -5, 200]\n",
    "})\n",
    "\n",
    "# Define the bins for age groups\n",
    "age_bins = [1, 16, 27, 66, 115]\n",
    "\n",
    "# Labels for the age groups\n",
    "age_labels = ['1-15', '16-26', '27-65', '65+']  \n",
    "\n",
    "# Apply corrections for age values outside the valid range (negative, 0, or greater than 115)\n",
    "people_cleaned['age'] = people_cleaned['age'].apply(lambda x: np.nan if x < 1 or x > 115 else x)\n",
    "\n",
    "# Apply pd.cut() to create a new 'age_group' column\n",
    "people_cleaned['age_group'] = pd.cut(people_cleaned['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "# Print the first few rows to verify the new grouping\n",
    "print(people_cleaned[['age', 'age_group']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f80a22",
   "metadata": {},
   "source": [
    "Drivers license state and class is not relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390cc5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned.drop(columns=['person_id',\n",
    "                      'person_type',\n",
    "                      'vehicle_id',\n",
    "                      'drivers_license_state', \n",
    "                      'drivers_license_class',\n",
    "                      'city', \n",
    "                      'state', \n",
    "                      'zipcode',\n",
    "                      'hospital', \n",
    "                      'crash_date',\n",
    "                      'seat_no',\n",
    "                      'ejection',\n",
    "                      'injury_classification',\n",
    "                      'driver_vision',\n",
    "                      'driver_action',\n",
    "                      'physical_condition',\n",
    "                      'bac_result',\n",
    "                      'age', \n",
    "                      'safety_equipment',\n",
    "                      'safety_equipment_category'\n",
    "                     ], inplace = True)\n",
    "people_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827413b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in people_cleaned.columns:\n",
    "    print(people_cleaned[feature].value_counts())\n",
    "    print()\n",
    "    print(\"-\" * 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92473101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the specified columns to string type\n",
    "people_cleaned[['age_group', 'airbag_deployed', 'sex']] = people_cleaned[['age_group', 'airbag_deployed', 'sex']].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642cc1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(people_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0fb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac69f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "people[['age', 'sex', 'airbag_deployed']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3955fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416b24e",
   "metadata": {},
   "source": [
    "The goal of aggregating the people_cleaned dataset is to handle the many-to-one relationship between the people_cleaned and crashes_cleaned datasets. Each crash_record_id in crashes_cleaned may have multiple associated records in people_cleaned, as a single crash may involve multiple people. Since our focus is on predicting the severity of crashes using the severity_category (the target variable in crashes_cleaned), we need to aggregate the people data to ensure each crash record has only one corresponding row of data.\n",
    "\n",
    "The aggregation process involves grouping the people_cleaned dataset by crash_record_id, which is the shared key between the two datasets. For features like sex, age_group, and airbag_deployed, we use the most frequent value for each crash. In cases where there is a tie (e.g., multiple values with the same frequency), we resolve the tie by selecting the value with the highest count using the idxmax() function on the value counts of each group. This ensures consistency and avoids ambiguity in cases of a tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_tie(x):\n",
    "    \"\"\"\n",
    "    Resolves ties in categorical data by selecting the most frequent value.\n",
    "    If the series is empty or contains only NaNs, returns NaN.\n",
    "    \"\"\"\n",
    "    if x.isna().all():\n",
    "        return np.nan  # or another placeholder value\n",
    "    return x.value_counts().idxmax()  # Pick the most frequent value in case of a tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93abad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the people data by crash_record_id\n",
    "\n",
    "# First, optimize tie resolution and avoid unnecessary grouping\n",
    "sex_aggregated = people_cleaned.groupby('crash_record_id')['sex'].apply(resolve_tie).reset_index()\n",
    "age_group_aggregated = people_cleaned.groupby('crash_record_id')['age_group'].apply(lambda x: x.mode().iloc[0]).reset_index()\n",
    "airbag_aggregated = people_cleaned.groupby('crash_record_id')['airbag_deployed'].apply(resolve_tie).reset_index()\n",
    "\n",
    "# Merge the results\n",
    "people_aggregated = sex_aggregated.merge(age_group_aggregated, on='crash_record_id').merge(airbag_aggregated, on='crash_record_id')\n",
    "\n",
    "# Preview the aggregated people data\n",
    "people_aggregated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decca6c8",
   "metadata": {},
   "source": [
    "### 2.3 Vehicles\n",
    "\n",
    "2.3.1 Preview Data\n",
    "\n",
    "2.3.2 Understand Structure\n",
    "\n",
    "2.3.3 Format Feature names and Row Values\n",
    "\n",
    "2.3.4 Drop features with overly high null values\n",
    "\n",
    "2.3.5 Check for duplicates\n",
    "\n",
    "2.3.6 inspect remaining features\n",
    "\n",
    "\n",
    "2.3.6.1 reduce feature cardinality with bucketing\n",
    "\n",
    "2.3.7 remove unuseful features\n",
    "\n",
    "2.3.8 Convert data types\n",
    "\n",
    "2.3.9 remove remaining nulls\n",
    "\n",
    "2.3.10 Merge Preparation: Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeccb115",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "* Steps:\n",
    "* **Preview Data**: `.head()`\n",
    "\n",
    "\n",
    "* **Understand Structure**: `.info()`\n",
    "\n",
    "\n",
    "* **Format Feature names and Row Values**: `.lower()`\n",
    "\n",
    "\n",
    "* **Drop features with overly high null values**: `.isna().sum()/ len(df)` for percentage of nulls for each feature\n",
    "\n",
    "\n",
    "* **Check for duplicates**: `.duplicated().sum()`\n",
    "\n",
    "\n",
    "* **inspect remaining features**: `.value_counts()`; \n",
    "\n",
    "    * make intentional decisions to keep or drop using `.value_counts()` distribution and domain knowledge; \n",
    "    * make note of any features to keep that will need cleaning/cardinality reduction/etc.\n",
    "\n",
    "\n",
    "* **remove unuseful features**: \n",
    "\n",
    "    * `.drop()` for list of features deemed not useful for analysis; \n",
    "    * store trimmed df as 'df_name_cleaned'\n",
    "\n",
    "\n",
    "* **reduce feature cardinality with bucketing**:\n",
    "\n",
    "    * 'safety_equipment' to 'safety_equipment_category'\n",
    "    * 'age' to 'age_group'\n",
    " \n",
    " \n",
    "* **Convert data types**: \n",
    "\n",
    "    * stored data types to reflect true data types \n",
    "    * (text as string, categorical variables as categories, numeric variables as int, ect.) \n",
    " \n",
    "* **remove remaining nulls**: `.dropna()`\n",
    "\n",
    "\n",
    "* **Merge Preparation: Aggregation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada03efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = pd.read_csv('./data/vehicles.csv', low_memory = True)\n",
    "vehicles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29bb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0956ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles.columns = vehicles.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a6f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string values in object columns to lowercase\n",
    "for col in vehicles.select_dtypes(include='object').columns:\n",
    "    vehicles[col] = vehicles[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f4e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting all features with 90% or more of its values are null\n",
    "high_null_features = vehicles.columns[(vehicles.isna().sum() / len(vehicles) * 100) >= 90]\n",
    "high_null_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of features with 90% or more null values\n",
    "high_null_features_list = list(high_null_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6615948",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned = vehicles.drop(columns=high_null_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates based on crash_unit_id, crash_record_id, and vehicle_id\n",
    "vehicles_cleaned[vehicles_cleaned.duplicated(subset=['crash_unit_id', 'crash_record_id', 'vehicle_id'], keep=False)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3d007",
   "metadata": {},
   "source": [
    "The output tells us that there are no duplicate rows in the vehicles_cleaned DataFrame based on the specified subset of columns: crash_unit_id, crash_record_id, and vehicle_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb981c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['num_passengers'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c5d7d",
   "metadata": {},
   "source": [
    "This is redundant information. This information does not include the driver, but this information is captured in occupant count. will drop this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6401e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['unit_no'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34ff51",
   "metadata": {},
   "source": [
    "This is aftermath. unhelpful. remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53baee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vehicles_cleaned['unit_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bd511",
   "metadata": {},
   "source": [
    "Most of the values are drivers or parked cars. this will not be useful for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadbd87d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vehicles_cleaned['make'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb3200",
   "metadata": {},
   "source": [
    "high cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2910fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['model'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d79c9f",
   "metadata": {},
   "source": [
    "This feels like it could be helpful, but many unknowns and 'other', and very high cardinality. The important information that we'd gain from this is already included in vehicle_type. So we can drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3fbf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['vehicle_defect'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658488d7",
   "metadata": {},
   "source": [
    "Most of the values are none or unknown. This will not be particularly useful for analysis. can drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e22ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['vehicle_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['travel_direction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d7678",
   "metadata": {},
   "source": [
    "Unhelpful for analysis. Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['maneuver'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca554801",
   "metadata": {},
   "source": [
    "This feature could be important as it has to do with what was happening prior to the crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['towed_i'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3af39",
   "metadata": {},
   "source": [
    "Aftermath; Unhelpful for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['occupant_cnt'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb282d0",
   "metadata": {},
   "source": [
    "It is unclear what the area_##_i features represent. They will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['first_contact_point'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da83898",
   "metadata": {},
   "source": [
    "This feature could indicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b38fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_features_to_drop = ['num_passengers', \n",
    "                            'crash_unit_id',\n",
    "                            'crash_date',\n",
    "                            'unit_type',\n",
    "                            'make', \n",
    "                            'model',\n",
    "                            'vehicle_id',\n",
    "                           'vehicle_defect',\n",
    "                           'unit_no',\n",
    "                           'lic_plate_state',\n",
    "                            'vehicle_year',\n",
    "                           'vehicle_use',\n",
    "                           'travel_direction',\n",
    "                           'towed_i',\n",
    "                            'area_01_i',\n",
    "                           'area_02_i', \n",
    "                            'area_05_i',\n",
    "                            'area_06_i',\n",
    "                            'area_07_i',\n",
    "                            'area_08_i',\n",
    "                            'area_10_i',\n",
    "                            'area_11_i',\n",
    "                            'area_12_i',\n",
    "                            'area_99_i', \n",
    "                           'first_contact_point']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90934c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned = vehicles_cleaned.drop(columns=vehicle_features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(vehicles_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffdd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map the original vehicle types to more specific categories\n",
    "vehicle_type_mapping = {\n",
    "    'passenger': 'Passenger Vehicles',\n",
    "    'sport utility vehicle (suv)': 'SUVs',\n",
    "    'van/mini-van': 'Passenger Vehicles',\n",
    "    'pickup': 'Trucks',\n",
    "    'truck - single unit': 'Trucks',\n",
    "    'single unit truck with trailer': 'Trucks',\n",
    "    'other': 'Other',\n",
    "    'bus over 15 pass.': 'Buses',\n",
    "    'bus up to 15 pass.': 'Buses',\n",
    "    'tractor w/ semi-trailer': 'Trucks',\n",
    "    'tractor w/o semi-trailer': 'Trucks',\n",
    "    'motorcycle (over 150cc)': 'Motorcycles',\n",
    "    'other vehicle with trailer': 'Other',\n",
    "    'autocycle': 'Motorcycles',\n",
    "    'moped or motorized bicycle': 'Motorcycles',\n",
    "    'motor driven cycle': 'Motorcycles',\n",
    "    'all-terrain vehicle (atv)': 'Recreational/Off-Highway Vehicles',\n",
    "    'farm equipment': 'Farm and Specialized Equipment',\n",
    "    '3-wheeled motorcycle (2 rear wheels)': 'Motorcycles',\n",
    "    'recreational off-highway vehicle (rov)': 'Recreational/Off-Highway Vehicles',\n",
    "    'snowmobile': 'Recreational/Off-Highway Vehicles',\n",
    "    'unknown/na': np.nan  # Set 'unknown/na' to NaN\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'vehicle_type' column\n",
    "vehicles_cleaned['vehicle_category'] = vehicles_cleaned['vehicle_type'].map(vehicle_type_mapping)\n",
    "\n",
    "# Check the value counts for the new grouped categories\n",
    "vehicles_cleaned['vehicle_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with 'Recreational/Off-Highway Vehicles' and 'Farm and Specialized Equipment'\n",
    "vehicles_cleaned = vehicles_cleaned[~vehicles_cleaned['vehicle_category'].isin(['Recreational/Off-Highway Vehicles', 'Farm and Specialized Equipment'])]\n",
    "\n",
    "# Check the value counts after removing those categories\n",
    "vehicles_cleaned['vehicle_category'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa86411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the maneuver mapping to treat 'unknown/na' as NaN\n",
    "maneuver_mapping = {\n",
    "    'straight ahead': 'Standard Movement',\n",
    "    'slow/stop in traffic': 'Standard Movement',\n",
    "    'passing/overtaking': 'Standard Movement',\n",
    "    'unknown/na': np.nan,  # Set 'unknown/na' to NaN\n",
    "    \n",
    "    'parked': 'Reversing/Stopping',\n",
    "    'entering traffic lane from parking': 'Reversing/Stopping',\n",
    "    'starting in traffic': 'Reversing/Stopping',\n",
    "    \n",
    "    'turning left': 'Turn/Change of Direction',\n",
    "    'turning right': 'Turn/Change of Direction',\n",
    "    'u-turn': 'Turn/Change of Direction',\n",
    "    'changing lanes': 'Turn/Change of Direction',\n",
    "    'turning on red': 'Turn/Change of Direction',\n",
    "    \n",
    "    'backing': 'Reversing/Stopping',\n",
    "    'avoiding vehicles/objects': 'Avoidance/Emergency Response',\n",
    "    'skidding/control loss': 'Avoidance/Emergency Response',\n",
    "    'negotiating a curve': 'Avoidance/Emergency Response',\n",
    "    \n",
    "    'leaving traffic lane to park': 'Reversing/Stopping',\n",
    "    'enter from drive/alley': 'Reversing/Stopping',\n",
    "    \n",
    "    'driving wrong way': 'Special Cases',\n",
    "    'diverging': 'Special Cases',\n",
    "    'driverless': 'Special Cases',\n",
    "    'disabled': 'Special Cases',\n",
    "    \n",
    "    'other': 'Special Cases',\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'maneuver' column\n",
    "vehicles_cleaned['maneuver_category'] = vehicles_cleaned['maneuver'].map(maneuver_mapping)\n",
    "\n",
    "# Check the value counts for the new 'maneuver_category'\n",
    "vehicles_cleaned['maneuver_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef61508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 0, 99, and negative values with NaN in 'occupant_cnt' for future dropping\n",
    "vehicles_cleaned['occupant_cnt'] = vehicles_cleaned['occupant_cnt'].replace([0, 99], np.nan)\n",
    "\n",
    "# Define bins for the categories (including the 20-98 range for Very Large Group)\n",
    "bins = [1, 4, 8, 19, 98, float('inf')]  # Adjusted to include 20-98 for Very Large Group\n",
    "labels = ['Single Occupancy', 'Small Group', 'Medium Group', 'Large Group', 'Very Large Group']  # 5 labels for 5 bins\n",
    "\n",
    "# Use pd.cut to categorize the 'occupant_cnt' column based on the bins\n",
    "vehicles_cleaned['occupant_category'] = pd.cut(\n",
    "    vehicles_cleaned['occupant_cnt'], \n",
    "    bins=bins, \n",
    "    labels=labels, \n",
    "    right=True, \n",
    "    include_lowest=False  # Exclude 0 from Single Occupancy\n",
    ")\n",
    "\n",
    "# Handle the NaN values for 'occupant_category' (those rows with invalid occupant_cnt, such as 0 or 99)\n",
    "vehicles_cleaned['occupant_category'] = vehicles_cleaned['occupant_category'].where(\n",
    "    vehicles_cleaned['occupant_category'].notna(), np.nan\n",
    ")\n",
    "\n",
    "# Check the categories to ensure the correct bucketing\n",
    "vehicles_cleaned['occupant_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Very Large Group' rows since it contains zero values\n",
    "vehicles_cleaned = vehicles_cleaned[vehicles_cleaned['occupant_category'] != 'Very Large Group']\n",
    "\n",
    "# Drop 'Very Large Group' from the categorical data if it still exists\n",
    "vehicles_cleaned['occupant_category'] = vehicles_cleaned['occupant_category'].cat.remove_categories('Very Large Group')\n",
    "\n",
    "# Check the categories to ensure the correct bucketing\n",
    "vehicles_cleaned['occupant_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c33835",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb3693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles['occupant_cnt'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles['occupant_cnt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce678482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'occupant_category' and 'occupant_cnt' column\n",
    "vehicles_cleaned = vehicles_cleaned.drop(columns=['occupant_category', 'occupant_cnt', 'vehicle_type', 'maneuver'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0570c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any null (NaN) values\n",
    "vehicles_cleaned = vehicles_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c14b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a0ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e86285",
   "metadata": {},
   "source": [
    "Similarly, the vehicles_cleaned dataset also has a many-to-one relationship with crashes_cleaned, where each crash_record_id in crashes_cleaned can have multiple associated vehicle records. As with the people_cleaned data, we need to aggregate the vehicle data to ensure that each crash record has a corresponding single row. The aggregation will allow us to focus on features like vehicle_category and other vehicle-specific attributes that might affect crash severity.\n",
    "\n",
    "By grouping the vehicles_cleaned dataset by crash_record_id, we can apply the same aggregation logic as with the people data. This ensures that we retain the most important vehicle-specific features while also maintaining a consistent one-to-one relationship between crashes_cleaned and the aggregated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the vehicles data by crash_record_id\n",
    "vehicles_aggregated = vehicles_cleaned.groupby('crash_record_id').agg({\n",
    "    'vehicle_category': resolve_tie,  # Resolve tie by choosing the most frequent vehicle category\n",
    "    'maneuver_category': resolve_tie,  # Resolve tie by choosing the most frequent maneuver category\n",
    "}).reset_index()\n",
    "\n",
    "# View the aggregated vehicles data\n",
    "vehicles_aggregated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db80a00",
   "metadata": {},
   "source": [
    "### Merging `crashes_cleaned`, `people_cleaned`, & `vehicles_cleaned`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d729cb6",
   "metadata": {},
   "source": [
    "Relationships Between Tables and Justification for Merging\n",
    "\n",
    "1. Relationship Between crashes_cleaned and people_cleaned\n",
    "\t•\tcrash_record_id is the primary key in crashes_cleaned and appears in people_cleaned.\n",
    "\t•\tEach crash in crashes_cleaned can involve multiple people (drivers, passengers, pedestrians).\n",
    "\n",
    "This means the relationship is:\n",
    "\t•\tOne-to-Many: One crash (crashes_cleaned) can have many people (people_cleaned) associated with it.\n",
    "\n",
    "2. Relationship Between crashes_cleaned and vehicles_cleaned\n",
    "\t•\tcrash_record_id is the primary key in crashes_cleaned and appears in vehicles_cleaned.\n",
    "\t•\tEach crash in crashes_cleaned can involve multiple vehicles.\n",
    "\n",
    "This means the relationship is:\n",
    "\t•\tOne-to-Many: One crash (crashes_cleaned) can have many vehicles (vehicles_cleaned) associated with it.\n",
    "\n",
    "3. Relationship Between people_cleaned and vehicles_cleaned\n",
    "\t•\tBoth tables are linked via crash_record_id, but they describe different entities.\n",
    "\t•\tPeople (people_cleaned) and vehicles (vehicles_cleaned) may not have a direct relationship unless there’s another shared identifier (e.g., vehicle_id or person_id).\n",
    "\n",
    "This means the relationship is:\n",
    "\t•\tMany-to-Many: Many people can be in many vehicles within the same crash. (However, this relationship is indirectly expressed through crash_record_id.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19805643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes using an inner join\n",
    "merged_df = crashes_cleaned.merge(people_aggregated, on='crash_record_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b54780",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.merge(vehicles_aggregated, on='crash_record_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dfa367",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f680bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the columns (except 'crash_record_id') to category type\n",
    "merged_df[[col for col in merged_df.columns if col != 'crash_record_id']] = merged_df[[col for col in merged_df.columns if col != 'crash_record_id']].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the changes\n",
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c35b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes.columns = crashes.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba9e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string values in object columns to lowercase\n",
    "for col in crashes.select_dtypes(include='object').columns:\n",
    "    crashes[col] = crashes[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49499a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['severity_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate crash_record_id values\n",
    "merged_df[merged_df.duplicated(subset='crash_record_id', keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6465737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each column in the DataFrame and print value counts for each feature\n",
    "for column in merged_df.columns:\n",
    "    print(f\"Value counts for {column}:\")\n",
    "    print(merged_df[column].value_counts())\n",
    "    print(\"-\" * 50)  # Optional separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d517dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3922b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns you want to clean\n",
    "columns_to_clean = ['airbag_deployed', 'roadway_surface_cond', 'lighting_condition']  # replace with your actual column names\n",
    "\n",
    "# Iterate over each column in the list\n",
    "for column in columns_to_clean:\n",
    "    # Check for unwanted values in the current column and remove rows\n",
    "    merged_df = merged_df[~merged_df[column].str.contains(\n",
    "        'unknown|not applicable|other object|unknown/other', case=False, na=False)]\n",
    "\n",
    "# Verify the changes\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b100c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the DataFrame after removal\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b5064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['severity_category'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa9d5c",
   "metadata": {},
   "source": [
    "From the above output, we see that the classes are greatly imbalanced. This will be something I will need to address during the modeling phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ca7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping classes into two groups for binary classification: 0 and 1\n",
    "merged_df.severity_category.replace({\n",
    "    'Serious' : 1,\n",
    "    'Non-serious' : 0},\n",
    "    inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099e90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocessing the data with OneHotEncoder for categorical features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = merged_df.drop(columns=['severity_category'], axis = 1)  # Drop the target column to get the features\n",
    "y = merged_df['severity_category']\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)  # sparse=False to return an array instead of a sparse matrix\n",
    "\n",
    "# Fit the encoder and transform the categorical features\n",
    "x_ohe = ohe.fit_transform(x)\n",
    "\n",
    "# Convert the transformed data into a DataFrame for easy column naming\n",
    "ohe_df = pd.DataFrame(x_ohe, columns=ohe.get_feature_names_out(x.columns))\n",
    "\n",
    "# Step 2: Train a Decision Tree Classifier\n",
    "mgmt_tree = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "mgmt_tree.fit(ohe_df, y)\n",
    "\n",
    "# Step 3: Use feature_importances_ to get the importance of features\n",
    "feature_importances = mgmt_tree.feature_importances_\n",
    "\n",
    "# Step 4: Sort and visualize feature importances\n",
    "sorted_indices = feature_importances.argsort()[::-1]  # Sort in descending order\n",
    "sorted_feature_names = ohe_df.columns[sorted_indices]\n",
    "sorted_importances = feature_importances[sorted_indices]\n",
    "\n",
    "# Create a bar plot of the feature importances\n",
    "sns.set(rc={'figure.figsize':(11.7, 8.27)})  # Set the plot size\n",
    "sns.barplot(x=sorted_importances, y=sorted_feature_names)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Feature Importances from Decision Tree Classifier', fontsize=16)\n",
    "plt.xlabel('Importance Score', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "\n",
    "# Increase tick mark size\n",
    "plt.tick_params(axis='both', which='major', labelsize=11)\n",
    "\n",
    "# Save the plot if desired and display it\n",
    "plt.savefig(\"./feature_importances.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f60f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
